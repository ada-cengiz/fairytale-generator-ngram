{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fairytale Generator Project"
      ],
      "metadata": {
        "id": "FUkz1dJe3C1A"
      },
      "id": "FUkz1dJe3C1A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We wanted to make a fairytale generator mostly based on N-gram models. Our primary goal is to provide parents who struggle with creative storytelling with a model that can help generate customizable fairytales for their children every evening.\n",
        "\n",
        "We used the tools we have covered in lectures and also implemented additional codes to be able to generate more complex structures."
      ],
      "metadata": {
        "id": "e1FhfwUc3Ky8"
      },
      "id": "e1FhfwUc3Ky8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) First, we chose the Grimms' Fairy Tales by Jacob Grimm and Wilhelm Grimm as our data set as this book is a renowned collection of folk tales compiled in the early 19th century.\n",
        "\n",
        "In these lines of codes which we have covered in lectures, we retrieve the book from Project Gutenberg's website. It opens the URL of the book, reads the data from the URL and decodes it into a readable string.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3jnXatcK3xAu"
      },
      "id": "3jnXatcK3xAu"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "caff3549",
      "metadata": {
        "id": "caff3549"
      },
      "outputs": [],
      "source": [
        "#Retrieve the book using its link and decode it via appropriate encoding.\n",
        "\n",
        "from urllib.request import urlopen # for reading websites\n",
        "\n",
        "url = \"https://www.gutenberg.org/ebooks/2591.txt.utf-8\" #declare the url\n",
        "f = urlopen(url) #\n",
        "grimms_fairytale = f.read().decode('utf-8-sig') #read the file with (utf-8) encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Here we are removing the introduction and conclusion of the Project Gutenberg to avoid generating them into final text."
      ],
      "metadata": {
        "id": "QZFG0QlY5yL7"
      },
      "id": "QZFG0QlY5yL7"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7a2f82ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a2f82ce",
        "outputId": "37fd72b7-5362-4d35-e809-e9d12e652e5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "#Get rid of the initial part of the book where Gutenberg project is explained in detail.\n",
        "\n",
        "import nltk #imports nltk library for text processing\n",
        "nltk.download('punkt_tab') #downloads tokenizer from NLTK.\n",
        "\n",
        "start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK GRIMMS' FAIRY TALES ***\"\n",
        "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK GRIMMS' FAIRY TALES ***\"\n",
        "\n",
        "# Split the text\n",
        "parts = grimms_fairytale.split(start_marker) # Splits the entire 'grimms_fairytale' text into a list of strings using the 'start_marker' as the dividing point.\n",
        "if len(parts) > 1: #Checks if the 'start_marker' was found in the text.\n",
        "    story = parts[1].split(end_marker)[0].strip() #Extracts the text between the start and end markers, then removes whitespace.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Before implementing this code block, we tried to use the cleanup technique from the lectures but this worked better as the initial cleanup. This code block helps us split the large story text into individual tales and performs the cleanup.\n",
        "It uses RegEx to find story boundaries."
      ],
      "metadata": {
        "id": "y01AyV9-7XWi"
      },
      "id": "y01AyV9-7XWi"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ba424b7d",
      "metadata": {
        "id": "ba424b7d"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "raw_stories = re.split(r'\\n[A-Z][A-Z\\s\\-]+\\.?\\n', story) #splits the story into potential individual tales using uppercase title patterns as dividers\n",
        "raw_stories = [s.strip() for s in raw_stories if len(s.strip()) > 300]  #remove junk, filter out short, non-story parts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) This block is word tokenization using NLTK"
      ],
      "metadata": {
        "id": "pc3up4L39TJx"
      },
      "id": "pc3up4L39TJx"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e910794c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e910794c",
        "outputId": "a57455c1-53c9-4202-93c0-563895034112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize(text): # Defines a function named 'tokenize' that takes one argument, 'text'.\n",
        "    return nltk.word_tokenize(text) # Uses NLTK's 'word_tokenize' function to split the input 'text' into a list of individual word tokens and returns this list."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Thanks to your feedback we were able to use spaCy for advanced text preprocessing. It helps us with named entity recognition and replacing them with placeholders. It was hard to to this without spaCy since spaCy is better with named entity recognition.\n",
        "\n",
        "Also based on your feedback we tried to train different multiple models for each setting."
      ],
      "metadata": {
        "id": "sYOTTNPN9978"
      },
      "id": "sYOTTNPN9978"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "31e97fc0",
      "metadata": {
        "id": "31e97fc0"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import re #regex for pattern matching in text\n",
        "import nltk #nltk for text processing\n",
        "import string #string module = collection of string constants including punctuation.\n",
        "\n",
        "#Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\") #this line loads small English model from spaCy for named entity recognition\n",
        "\n",
        "#Hero role words to normalize\n",
        "hero_types_list = [\"princess\", \"knight\", \"wizard\", \"hunter\", \"peasant\", \"witch\",\"king\",\"prince\",\"queen\"] #we have list of common hero types that we encounter in fairytales, later to be replaced by placeholders.\n",
        "\n",
        "#Define setting keywords. These lines of code creates a dictionary where keys are setting names and values are lists of keywords associated with that setting\n",
        "setting_keywords = {\n",
        "    \"forest\": [\"forest\", \"woods\", \"tree\"],\n",
        "    \"castle\": [\"castle\", \"tower\", \"palace\"],\n",
        "    \"village\": [\"village\", \"town\", \"hamlet\"],\n",
        "    \"cave\": [\"cave\", \"rock\", \"underground\"]\n",
        "}\n",
        "\n",
        "#Initialize setting-specific corpus\n",
        "#here we create seperate empty lists which will become specialized corpuses for each setting.\n",
        "corpus_by_setting = {s: [] for s in setting_keywords.keys()}\n",
        "\n",
        "#This function classifies each individual story into one of the defined settings above based on the keywords in the text\n",
        "def detect_setting(text):\n",
        "    text = text.lower() #ocnverts input text to lowercase to ensure case-insensitive matching\n",
        "    for setting, words in setting_keywords.items(): #itirates thorugh ecah setting\n",
        "        if any(word in text for word in words): #checks if any keywords are present in the current setting\n",
        "            return setting #returns detected setting based on the keyword\n",
        "    return \"unknown\" #if no keywords are found returns unknown\n",
        "\n",
        "#Replace real entities with placeholders for customization\n",
        "def replace_with_placeholders(text):\n",
        "    doc = nlp(text) #processes the input text using spaCy. This creates\n",
        "    modified = text #new string with altered placeholders\n",
        "\n",
        "    for ent in doc.ents: #this itirates through each detected named entity in doc object\n",
        "        if ent.label_ == \"PERSON\": #checks if the current entity's label indicates it is person\n",
        "            modified = modified.replace(ent.text, \"[PERSON_1]\") #if it is a person this line replaces that entity in modified with the placeholder [PERSON_1]\n",
        "        elif ent.label_ in {\"GPE\", \"LOC\"}: #this line is for the setting, it checks whether the entity's label indicates any location\n",
        "            modified = modified.replace(ent.text, \"[SETTING]\") #if it is a location then it replaces that entity in modified with placeholder [SETTING]\n",
        "\n",
        "    for role in hero_types_list:\n",
        "        modified = re.sub(rf\"\\b{role}\\b\", \"[HERO_TYPE]\", modified, flags=re.IGNORECASE) # Uses a regex to replace all whole-word occurrences of the current 'role' in modified with the placeholder [HERO_TYPE]\n",
        "\n",
        "    return modified #returns the whole modified string containing all the customization choices\n",
        "\n",
        "#Process stories using spaCy-based placeholder replacement\n",
        "for tale in raw_stories:\n",
        "    setting = detect_setting(tale) #Calls detect_setting to determine the dominant setting of the current tale.\n",
        "    if setting != \"unknown\": #checks if a specific settin was detected\n",
        "        tale = replace_with_placeholders(tale) #if a setting is known, this line applies replace_with_placeholders to tale\n",
        "        sents = nltk.sent_tokenize(tale) #splits modified tale into individual sentences using sentence tokenizer\n",
        "        tokens = [nltk.word_tokenize(sent) for sent in sents]\n",
        "        for sentence in tokens:\n",
        "            cleaned = [w.lower() for w in sentence if w not in string.punctuation] #tokenized words cleaning\n",
        "            corpus_by_setting[setting] += ['<s>'] + cleaned + ['</s>'] #appends cleaned words of the sentence along with sentence start and end markers to the list associated with its setting\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Count occurrences of n-grams\n",
        "from collections import Counter\n",
        "\n",
        "#Initialize empty dictionaries to hold the n-gram models for each setting\n",
        "unigrams_by_setting = {}   #(single words)\n",
        "bigrams_by_setting = {}    #(pairs of words)\n",
        "trigrams_by_setting = {}   #(triplets of words)\n",
        "\n",
        "#Loop over each setting and its associated tokenized corpus\n",
        "for setting, corpus in corpus_by_setting.items():\n",
        "\n",
        "    # Count the frequency of each individual word (unigram) in the corpus\n",
        "    unigrams_by_setting[setting] = Counter(corpus)\n",
        "\n",
        "    # Generate a list of bigrams by pairing each word with the next\n",
        "    #Count the frequency of each bigram and store it\n",
        "    bigrams = [(corpus[i], corpus[i+1]) for i in range(len(corpus) - 1)]\n",
        "    bigrams_by_setting[setting] = Counter(bigrams)\n",
        "\n",
        "    # Generate a list of trigrams by pairing each word with the next two\n",
        "    # Count the frequency of each trigram and store it\n",
        "    trigrams = [(corpus[i], corpus[i+1], corpus[i+2]) for i in range(len(corpus) - 2)]\n",
        "    trigrams_by_setting[setting] = Counter(trigrams)\n"
      ],
      "metadata": {
        "id": "FuJBCx6-nVOY"
      },
      "id": "FuJBCx6-nVOY",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Here we add placeholder tokens like PERSON1TOKEN and HEROTYPETOKEN to make the model focus on story structure instead of specific names.\n",
        "\n",
        "We learned in class that using consistent tokens helps the model generalize better. We also added some fake lines about the character and setting throughout the text to reinforce common fairy tale patterns during training."
      ],
      "metadata": {
        "id": "Shj7hM33hA1B"
      },
      "id": "Shj7hM33hA1B"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "20fba955",
      "metadata": {
        "id": "20fba955"
      },
      "outputs": [],
      "source": [
        "\n",
        "def insert_fake_tokens(text, setting):\n",
        "    #Keywords used for training only\n",
        "    hero_names = [\"Hansel\", \"Gretel\", \"Jack\", \"Aria\", \"Finn\", \"Luna\", \"Kai\", \"Crabb\"]\n",
        "    hero_types = [\"princess\", \"knight\", \"wizard\", \"hunter\", \"peasant\", \"witch\",\"prince\"]\n",
        "    settings = [\"forest\", \"castle\", \"cave\", \"village\"]\n",
        "\n",
        "    #Injected text with placeholders\n",
        "    injected_lines = (\n",
        "        f\"PERSON1TOKEN the HEROTYPETOKEN lived in the {setting}. \" #first placeholder sentence\n",
        "        f\"Every day, PERSON1TOKEN wandered the {setting}. \"        #second placeholder sentence\n",
        "        f\"Everyone in the {setting} knew PERSON1TOKEN was a brave HEROTYPETOKEN. \" #third placeholder sentence\n",
        "    ) * 3 #repeats the placeholder sentences 3 times\n",
        "\n",
        "    #Periodically insert the fake lines\n",
        "    chunks = text.split(\"\\n\\n\") #splits text into paragraphs\n",
        "    augmented_chunks = [] #initializes list for augmented chunks\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        if i % 5 == 0: #Checks if current chunk is every 5th one.\n",
        "            augmented_chunks.append(injected_lines) #inserts placeholder lines every 5th chunk\n",
        "        augmented_chunks.append(chunk) #adds augmented chunks to original chunk\n",
        "    text = \"\\n\\n\".join(augmented_chunks) #joins them back into single text\n",
        "\n",
        "    #Replace keywords with placeholders\n",
        "    import re\n",
        "    for name in hero_names:\n",
        "        text = re.sub(rf\"\\b{name}\\b\", \"PERSON1TOKEN\", text, flags=re.IGNORECASE) #itirate through every hero name and replace them with PERSON1TOKEN\n",
        "    for role in hero_types:\n",
        "        text = re.sub(rf\"\\b{role}\\b\", \"HEROTYPETOKEN\", text, flags=re.IGNORECASE) #itirate through every hero type and replace them with HEROTYPETOKEN\n",
        "    for s in sorted(settings, key=lambda s: -len(s)):\n",
        "        text = re.sub(rf\"\\b{s}\\b\", \"SETTINGTOKEN\", text, flags=re.IGNORECASE) #itirate through every setting and replace them with SETTINGTOKEN\n",
        "\n",
        "    return text #finally returns text with injected lines and replaced keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) Here we build n-gram models (bigrams, trigrams, and 4-grams) for each\n",
        " setting using MLE. This step trains our model to predict the next word based on the previous ones, depending on the setting of the story.  To handle unseen word combinations we apply Lidstone smoothing, which adds a small constant to all counts to avoid zero probabilities and ensure better generalization."
      ],
      "metadata": {
        "id": "gisefL5KksSa"
      },
      "id": "gisefL5KksSa"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a4e45a47",
      "metadata": {
        "id": "a4e45a47"
      },
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.probability import ConditionalFreqDist, ConditionalProbDist, LidstoneProbDist #imports probability classes\n",
        "\n",
        "models_by_setting = {} #dictionary to store models per setting\n",
        "\n",
        "for setting, corpus in corpus_by_setting.items(): #loops thorugh each setting and its tokens\n",
        "    bigrams = list(ngrams(corpus, 2))\n",
        "    trigrams = list(ngrams(corpus, 3))\n",
        "    fourgrams = list(ngrams(corpus, 4))\n",
        "\n",
        "    cf2 = ConditionalFreqDist((w1, w2) for w1, w2 in bigrams) #these lines counts frequencies for each ngram model\n",
        "    cf3 = ConditionalFreqDist(((w1, w2), w3) for w1, w2, w3 in trigrams)\n",
        "    cf4 = ConditionalFreqDist(((w1, w2, w3), w4) for w1, w2, w3, w4 in fourgrams)\n",
        "\n",
        "   # The '0.1' is the k parameter for Lidstone smoothing.\n",
        "    cp2 = ConditionalProbDist(cf2, LidstoneProbDist, 0.1)\n",
        "    cp3 = ConditionalProbDist(cf3, LidstoneProbDist, 0.1)\n",
        "    cp4 = ConditionalProbDist(cf4, LidstoneProbDist, 0.1)\n",
        "\n",
        "    models_by_setting[setting] = { #this dictionary stores all models for current setting\n",
        "        \"cp2\": cp2,\n",
        "        \"cp3\": cp3,\n",
        "        \"cp4\": cp4\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8)This is where we generate a fairy tale using the N-gram models, starting from a random template. This function implements a back-off strategy during word prediction. Here, the function tries to predict the next word using 4-grams, then falls back to 3-grams or bigrams if needed. As a final fallback, if no suitable N-gram context is available, it selects a random word from the entire setting-specific corpus. After generating text it replaces placeholders like [PERSON_1] with real character and setting values at the end to create a full, coherent story."
      ],
      "metadata": {
        "id": "BlLgCDpJnShb"
      },
      "id": "BlLgCDpJnShb"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "329db250",
      "metadata": {
        "id": "329db250"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from scipy import stats\n",
        "\n",
        "def custom_fairytale_with_format(hero=None, hero_type=None, setting=None, max_len=300): #defines story generation function\n",
        "    #Define multiple possible starter phrases with placeholders\n",
        "    starters = [  #here we made a list of possible fairytale starter templates based on conventional fairytale starters\n",
        "        [\"Once\", \"upon\", \"a\", \"time\", \"the\", \"[HERO_TYPE]\", \"[PERSON_1]\", \"was\", \"in\", \"the\", \"[SETTING]\"],\n",
        "        [\"Long\", \"ago\", \"in\", \"a\", \"[SETTING]\", \"there\", \"lived\", \"a\", \"[HERO_TYPE]\", \"named\", \"[PERSON_1]\"],\n",
        "        [\"In\", \"the\", \"heart\", \"of\", \"the\", \"[SETTING]\", \"[PERSON_1]\", \"the\", \"[HERO_TYPE]\", \"awoke\"],\n",
        "        [\"A\", \"[HERO_TYPE]\", \"called\", \"[PERSON_1]\", \"once\", \"journeyed\", \"through\", \"the\", \"[SETTING]\"],\n",
        "        [\"Many\", \"years\", \"ago,\", \"a\", \"[HERO_TYPE]\", \"named\", \"[PERSON_1]\", \"guarded\", \"the\", \"[SETTING]\"]\n",
        "    ]\n",
        "    #Choose one randomly\n",
        "    starter = random.choice(starters) # here we choose one of the starters randomly\n",
        "\n",
        "    words = starter[:] #initializes story words with starter\n",
        "    current_phrase = tuple(starter[-3:]) #sets initial ngram context\n",
        "\n",
        "    # Fetch language models and backup corpus\n",
        "    cp4 = models_by_setting[setting][\"cp4\"] #gets 4gram model for setting\n",
        "    cp3 = models_by_setting[setting][\"cp3\"] #gets 3gram model \"\"\n",
        "    cp2 = models_by_setting[setting][\"cp2\"] #gets 2gram model \"\"\n",
        "    fallback_corpus = corpus_by_setting[setting] #this gets full corpus for fallback\n",
        "\n",
        "    #Generate next word using a specific N-gram model\n",
        "    #If it fails moves down to smaller model this is backoff model\n",
        "    #(in case of no data for context)\n",
        "\n",
        "    used_trigrams = set() #this empty set is to keep track of trigrams already used to avoid immediate repetition\n",
        "\n",
        "    for _ in range(max_len - 3): #loops to generate words up to max_len\n",
        "        try: #4gram prediction\n",
        "            options = list(cp4[current_phrase].samples()) #next word options\n",
        "            probs = [cp4[current_phrase].prob(w) for w in options] #probabilities\n",
        "            next_word = random.choices(options, weights=probs, k=1)[0] #choose word for by probability\n",
        "        except: #fallback to 3gram incase 4gram fails\n",
        "            try: #3gram prediction\n",
        "                context = current_phrase[1:] #same as above but for 3grams\n",
        "                options = list(cp3[context].samples())\n",
        "                probs = [cp3[context].prob(w) for w in options]\n",
        "                next_word = random.choices(options, weights=probs, k=1)[0]\n",
        "            except: #fallback to 2grams incase 3gram fails\n",
        "                try: #2gram prediction\n",
        "                    context = (current_phrase[2],) #same as above but for 2grams\n",
        "                    options = list(cp2[context].samples())\n",
        "                    probs = [cp2[context].prob(w) for w in options]\n",
        "                    next_word = random.choices(options, weights=probs, k=1)[0]\n",
        "                except: #fallback to random word if all ngrams fail\n",
        "                    next_word = random.choice(corpus)\n",
        "\n",
        "        #Skip repeated trigrams\n",
        "        trigram = (current_phrase[0], current_phrase[1], next_word) #forms trigram for check\n",
        "        if trigram in used_trigrams: #check for repetition and skip if repeated and finally add trigram to used set\n",
        "          continue\n",
        "        used_trigrams.add(trigram)\n",
        "\n",
        "        words.append(next_word)\n",
        "        current_phrase = (current_phrase[1], current_phrase[2], next_word)\n",
        "\n",
        "    # Process words into sentences\n",
        "    sentences = [] # list to store finished sentencess\n",
        "    current_sentence = [] #list to build current sentence\n",
        "\n",
        "    for word in words:\n",
        "        if word == \"<s>\": #skip sentence start marker\n",
        "            continue\n",
        "        elif word == \"</s>\": #process sentence end marker\n",
        "          if current_sentence: #if sentence has words join words into sentence string\n",
        "            sentence = ' '.join(current_sentence)\n",
        "            if current_sentence: #redundant check\n",
        "                sentence = ' '.join(current_sentence) #join again and add punctuation if missing\n",
        "                if sentence[-1] not in '.!?':\n",
        "                    sentence += '.'\n",
        "                sentences.append(sentence) #add sentence to list and reset for next sentence\n",
        "                current_sentence = []\n",
        "        else:\n",
        "            current_sentence.append(word)\n",
        "\n",
        "    #Convert full text\n",
        "    story_text = ' '.join(sentences) #joins sentences into final text\n",
        "\n",
        "    starter = [word.replace(\"[HERO_TYPE]\", hero_type) #replace each in starter\n",
        "                    .replace(\"[PERSON_1]\", hero)\n",
        "                    .replace(\"[SETTING]\", setting) for word in starter]\n",
        "\n",
        "    #Ficxng broken placeholder variants\n",
        "    story_text = story_text.replace(\"person_1\", \"[PERSON_1]\") #correct lowercase for each\n",
        "    story_text = story_text.replace(\"hero_type\", \"[HERO_TYPE]\")\n",
        "    story_text = story_text.replace(\"setting\", \"[SETTING]\")\n",
        "\n",
        "    #Substitute placeholders after generation\n",
        "    #After training and generation, turns the template back into a real story\n",
        "    story_text = story_text.replace(\"[PERSON_1]\", hero) #replace each placeholder with actual names\n",
        "    story_text = story_text.replace(\"[HERO_TYPE]\", hero_type)\n",
        "    story_text = story_text.replace(\"[SETTING]\", setting)\n",
        "\n",
        "    #Clean spacing\n",
        "    story_text = story_text.replace(\" .\", \".\").replace(\" ,\", \",\").replace(\" !\", \"!\").replace(\" ?\", \"?\") #fix spacing\n",
        "    story_text = story_text.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\" ’\", \"’\")\n",
        "\n",
        "\n",
        "    import textwrap\n",
        "\n",
        "    def capitalize_sentences(text): #This function is for capitalizing the first letter of each sentence.\n",
        "      sentences = re.split(r'(?<=[.!?]) +', text)\n",
        "      return ' '.join(s.capitalize() for s in sentences)\n",
        "\n",
        "    # This part was added for a cleaner output. Since the input data \"Project Gutenberg\" uses typographic quotation\n",
        "    # marks, there were some singled out apostrophes appearing in the output that were out of place. The following two\n",
        "    # lines turn them into ASCII apostrophes.\n",
        "\n",
        "    story_text = story_text.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
        "    story_text = story_text.replace(\"“\", '\"').replace(\"”\", '\"')\n",
        "\n",
        "    story_text = capitalize_sentences(story_text)\n",
        "    wrapped = textwrap.fill(story_text, width=80) #The final output was appearing as a single line, we fixed this issue\n",
        "    #by using textwrap and setting the width to 80 characters so the output would be more readable especially in colab\n",
        "\n",
        "    return wrapped\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) This helper function returns the most likely next word based on a 4-gram model."
      ],
      "metadata": {
        "id": "Xvy2PzZ_pfGX"
      },
      "id": "Xvy2PzZ_pfGX"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "44566177",
      "metadata": {
        "id": "44566177"
      },
      "outputs": [],
      "source": [
        "def get_next_word(w1, w2, w3, cp4): #defines function to get next word\n",
        "    try: #most likely word\n",
        "        return cp4[(w1, w2, w3)].max() #returns most frequent next word\n",
        "    except: #catches if context not found\n",
        "        return \"</s>\"  # fallback token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10) This block defines a generate_story function. We use it to construct a story word-by-word using a 4gram model. It starts from a given seed, and then stops when maximum lenght is reached."
      ],
      "metadata": {
        "id": "LH_xn8Nb3xII"
      },
      "id": "LH_xn8Nb3xII"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1adb9d09",
      "metadata": {
        "id": "1adb9d09"
      },
      "outputs": [],
      "source": [
        "def generate_story(seed=[\"<s>\", \"<s>\", \"once\"], max_len=300): #defines a story generation function\n",
        "    story = seed[:] #initializes story with seed\n",
        "    cp4 = models_by_setting[setting][\"cp4\"] #gets 4gram model\n",
        "    while len(story) < max_len: #loops while story is short\n",
        "        w1, w2, w3 = story[-3], story[-2], story[-1]\n",
        "        next_word = get_next_word(w1, w2, w3) #predicts next word\n",
        "        story.append(next_word) #adds word to story\n",
        "        if next_word == \"</s>\" and len(story) > 100:\n",
        "          break #breaks if sentence is long enough\n",
        "\n",
        "    return \" \".join(story[3:])  # skip initial tokens and return joined story"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11) Finally this code block is for user interaction and story generation. This code block prompts the user for customization of hero name,type and setting then uses this input to generate customized fairytale"
      ],
      "metadata": {
        "id": "ymo3kYWK5kV1"
      },
      "id": "ymo3kYWK5kV1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5db5f6fc",
      "metadata": {
        "id": "5db5f6fc"
      },
      "outputs": [],
      "source": [
        "# Get user input\n",
        "user_hero = input(\"Enter the hero's name: \") #each line prompts the user for customized details and and stores them\n",
        "user_hero_type = input(\"Enter the hero's type (e.g., princess, knight, witch, peasant, wizard, hunter): \")\n",
        "user_setting = input(\"Enter the setting (e.g., forest, castle, cave, village): \")\n",
        "\n",
        "generated = custom_fairytale_with_format(setting=user_setting, hero=user_hero, hero_type=user_hero_type) #generation function passing user inputs\n",
        "\n",
        "print(generated)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}